## 딥러닝을 위한 기초 수학

인공지능은 예측하기 위해 **선형 회귀(linear regression)** 함수와 **로지스틱 회귀** 함수를 사용합니다.

**함수**: 입력값에 따라 변하는 수



### 일차 함수와 기울기와 y 절편

퍼셉트론은 일차함수

y = wx + b

w는 가중치이며 기울기


### 이차 함수와 최솟값

### 미분과 순간 변화율과 기울기

### 편미분

### 지수와 지수 함수

### 시그모이드 함수

### 로그와 로그 함수


## 선형 회귀의 정의

가장 훌륭한 예측선

- **독립변수**: 선형 회귀 함수의 x   
- **종속변수**: 선형 회귀 함수의 y

### 단순 선형 회귀

독립변수가 하나인 선형 회귀 = 일차 함수

y = ax + b

선형 회귀를 만드는 법: a와 b를 구함

**최소 제곱법**으로 a, b 구하기:

```math

```

```python
import numpy as np

x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])

mx = np.mean(x)
my = np.mean(y)

def top(x, mx, y, my):
    d = 0
    for i in range(len(x)):
        d += (x[i] - mx) * (y[i] - my)
    return d

divisor = sum([(i - mx)**2 for i in x])
dividend = top(x, mx, y, my)

a = dividend / divisor
b = my - mx * a
```
a = (x - x 평균) * (y - y평균)의 합 / (x - x 평균)**2의 합   
b = y의 평균 - x의 평균 * 기울기 a




### 다중 선형 회귀

독립변수가 여러 개인 선형 회귀


평균 제곱 오차
최소 제곱법을 이용해 기울기 a와 y 절편을 편리하게 구했지만, 이 공식만으로 앞으로 만나게 될 모든 상황을 해결하기는 어려움
여러 개의 입력을 처리하기에는 무리가 있기 때문임
예를 들어 앞서 살펴본 예에서는 변수가 ‘공부한 시간’ 하나뿐이지만, 2장에서 살펴본 폐암 수술 환자의 생존율 데이터를 보면 입력 데이터의 종류가 17개나 됨
딥러닝은 대부분 입력 값이 여러 개인 상황에서 이를 해결하기 위해 실행되기 때문에 기울기 a와 y 절편 b를 찾아내는 다른 방법이 필요함
![image](https://github.com/user-attachments/assets/9096e532-a3b3-4f46-ab01-8a8572ff1693)

평균 제곱 오차(MSE)로 a b 구하기

오차를 제곱하는 이유: 양수 오차와 음수 오차가 상쇄되어 오차가 사라질 수 있어서.
